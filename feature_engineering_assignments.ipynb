{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment questions\n"
      ],
      "metadata": {
        "id": "_oMBd0mzkP8l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "​1. What is a parameter?\n",
        "In machine learning, a parameter is a configuration variable that is internal to the model and whose value can be estimated from data. For example, in a linear regression model y = mx + c, the slope m and intercept c are parameters that the model learns during training.\n",
        "\n",
        "​2. What is correlation?\n",
        "Correlation is a statistical measure that expresses the extent to which two variables are linearly related. It describes how much one variable changes when the other one does. It is usually measured by the Pearson correlation coefficient, ranging from -1 to 1.\n",
        "\n",
        "​3. What does negative correlation mean?\n",
        "Negative correlation means that as one variable increases, the other variable tends to decrease. For example, as the speed of a car increases, the time taken to reach a destination decreases.\n",
        "\n",
        "​4. Define Machine Learning. What are the main components in Machine Learning?\n",
        "Machine Learning is a branch of AI that allows systems to learn from data and improve from experience without being explicitly programmed.\n",
        "Main Components:\n",
        "​Data: The raw information (features and labels).\n",
        "​Model/Algorithm: The mathematical structure (e.g., Linear Regression, Random Forest).\n",
        "​Loss Function: Measures how far the prediction is from the actual value.\n",
        "​Optimizer: Adjusts model parameters to minimize loss.\n",
        "\n",
        "\n",
        "​5. How does loss value help in determining whether the model is good or not?\n",
        "The loss value (or cost) quantifies the error of the model. A high loss indicates the model's predictions are far from the actual targets (poor model), while a low loss indicates the predictions are close to the targets (good model). During training, we aim to minimize this value.\n",
        "\n",
        "​6. What are continuous and categorical variables?\n",
        "​Continuous Variables: Numeric variables that can take any value within a range (e.g., height, temperature, price).\n",
        "​Categorical Variables: Variables that represent types or groups (e.g., gender, color, city names).\n",
        "\n",
        "​7. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "Machine learning models require numerical input, so categorical data must be encoded.\n",
        "Common Techniques:\n",
        "​Label Encoding: Assigning a unique integer to each category (0, 1, 2...).\n",
        "​One-Hot Encoding: Creating binary columns (0 or 1) for each category.\n",
        "\n",
        "​8. What do you mean by training and testing a dataset?\n",
        "​Training: Using a portion of the data to teach the model patterns.\n",
        "​Testing: Using a separate, unseen portion of data to evaluate how well the model generalizes to new information.\n",
        "\n",
        "​9. What is sklearn.preprocessing?\n",
        "It is a module in the Scikit-Learn library that provides functions to change raw feature vectors into a representation that is more suitable for the estimators. This includes scaling, normalization, and encoding.\n",
        "\n",
        "​10. What is a Test set?\n",
        "A test set is a subset of the original dataset that is kept hidden from the model during the training phase. It is used only at the very end to provide an unbiased evaluation of the final model's performance.\n",
        "\n",
        "​11. How do we split data for model fitting (training and testing) in Python?\n",
        "We use the train_test_split function from Scikit-Learn:"
      ],
      "metadata": {
        "id": "ho0VrYfNkajk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKB7wk_kjMN_"
      },
      "outputs": [],
      "source": [
        "#11​11. How do we split data for model fitting (training and testing) in Python?\n",
        "#sol-->We use the train_test_split function from Scikit-Learn:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. Splitting Data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming X is features and y is target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "5ffsPUkXlOGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#12.How do you approach a Machine Learning problem?"
      ],
      "metadata": {
        "id": "jb1u73o1lbPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the problem (Regression, Classification, etc.).\n",
        "​Data Collection.\n",
        "​Exploratory Data Analysis (EDA).\n",
        "​Data Preprocessing (Cleaning, Scaling, Encoding).\n",
        "​Feature Selection.\n",
        "​Model Selection and Training.\n",
        "​Evaluation and Tuning.\n",
        "​Deployment."
      ],
      "metadata": {
        "id": "tn0_j-omlltS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#​13. Why do we have to perform EDA before fitting a model to the data?"
      ],
      "metadata": {
        "id": "0WdaCp4wlbM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploratory Data Analysis (EDA) helps us understand the data's structure, identify outliers, find correlations, and detect missing values. It ensures that the data we feed into the model is clean and relevant."
      ],
      "metadata": {
        "id": "0o4KhIKDlpae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#​14. What is correlation?\n"
      ],
      "metadata": {
        "id": "mlr6dv5jlbJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation measures the linear relationship between two variables."
      ],
      "metadata": {
        "id": "da6S5SI0lxxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#​15. What does negative correlation mean?"
      ],
      "metadata": {
        "id": "1KIv7NzqlbG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It means as one variable increases, the other decreases."
      ],
      "metadata": {
        "id": "x7JcTekcl2m6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#​16. How can you find correlation between variables in Python?"
      ],
      "metadata": {
        "id": "wzJOrTs3l6Le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a Pandas DataFrame, you can use the .corr() method:"
      ],
      "metadata": {
        "id": "6n6kKlydl8H-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 16. Finding Correlation\n",
        "import pandas as pd\n",
        "df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n",
        "correlation_matrix = df.corr()\n",
        "print(correlation_matrix)"
      ],
      "metadata": {
        "id": "Xh0hPmKnl7ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#17. What is causation? Explain difference between correlation and causation with an example."
      ],
      "metadata": {
        "id": "89900iecmSEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation: Two things happen at the same time (e.g., Ice cream sales and sunburns both go up in summer).\n",
        "​Causation: One thing causes the other to happen (e.g., High heat causes ice cream to melt).\n",
        "Difference: Correlation does not imply causation. Ice cream sales don't cause sunburns; the sun (a third factor) causes both."
      ],
      "metadata": {
        "id": "e1eDqmwamUYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#18. What is an Optimizer? What are different types of optimizers?"
      ],
      "metadata": {
        "id": "-vAsP2apmR-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An optimizer is an algorithm that modifies the attributes of the neural network/model (like weights) to reduce the loss.\n",
        "​Gradient Descent: Updates weights in the opposite direction of the gradient.\n",
        "​Stochastic Gradient Descent (SGD): Updates weights for every single training example.\n",
        "​Adam (Adaptive Moment Estimation): Combines benefits of multiple optimization strategies."
      ],
      "metadata": {
        "id": "5mHGfRRKmYUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#​19. What is sklearn.linear_model?"
      ],
      "metadata": {
        "id": "VE7RqN6FmeLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is a module in Scikit-Learn that implements various generalized linear models, such as Linear Regression, Logistic Regression, and Ridge/Lasso Regression."
      ],
      "metadata": {
        "id": "flJZyjdpmdp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#​20. What does model.fit() do? What arguments must be given?"
      ],
      "metadata": {
        "id": "o-6Dm-hSmR7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model.fit() starts the training process. It calculates the optimal parameters for the model based on the data provided.\n",
        "Arguments: It usually requires (X_train, y_train) (the features and the corresponding target labels)."
      ],
      "metadata": {
        "id": "jZvg4L0lmnR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#21. What does model.predict() do? What arguments must be given?"
      ],
      "metadata": {
        "id": "EBkUnQLFmm_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model.predict() uses the trained model to generate predictions for new data.\n",
        "Arguments: It requires (X_test) or any new feature set that matches the shape of the training data."
      ],
      "metadata": {
        "id": "9XV3i9ETmqRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "​#22. What are continuous and categorical variables?"
      ],
      "metadata": {
        "id": "T5MK_nUgmsmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continuous are numeric/measurable; Categorical are groups/labels."
      ],
      "metadata": {
        "id": "oDPOh8qPmwal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#​23. What is feature scaling? How does it help in Machine Learning?"
      ],
      "metadata": {
        "id": "ixA6hhSwmz4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature scaling is the process of normalizing the range of independent variables (features).\n",
        "​Why it helps: Algorithms that use distance (like KNN or Gradient Descent) perform poorly if one feature has a range of 0-1 and another has a range of 0-1000. Scaling puts them on a level playing field."
      ],
      "metadata": {
        "id": "j1tzHIZdm2SW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#24. How do we perform scaling in Python?"
      ],
      "metadata": {
        "id": "OmIqeS7Jm3HZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 24. Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_train)"
      ],
      "metadata": {
        "id": "FUMmrpy6m3uF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#25What is sklearn.preprocessing?"
      ],
      "metadata": {
        "id": "7018S6gEm9wF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A module for scaling, encoding, and cleaning data."
      ],
      "metadata": {
        "id": "j22MvpLtnCwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#25. What is sklearn.preprocessing?\n"
      ],
      "metadata": {
        "id": "S1NLk3vanEnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A module for scaling, encoding, and cleaning data.\n"
      ],
      "metadata": {
        "id": "owhLOBhNnM4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#​26. How do we split data for model fitting?"
      ],
      "metadata": {
        "id": "Hm84u8-gnPTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Using train_test_split(X, y, test_size=0.2)."
      ],
      "metadata": {
        "id": "_SOJ6rJpnAWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#​27. Explain data encoding?"
      ],
      "metadata": {
        "id": "j5mOCDyznOy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data encoding is the process of converting categorical data (text/labels) into numerical values so that mathematical models can process them. Common methods include One-Hot Encoding (creating columns for each category) and Ordinal/Label Encoding"
      ],
      "metadata": {
        "id": "_bXf1EElm9f1"
      }
    }
  ]
}